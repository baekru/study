{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정적 크롤링 실습하기\n",
    "\n",
    "각종 금융 웹사이트에는 주가, 재무정보 등 우리가 원하는 대부분의 주식 정보가 제공되고 있으며, 크롤링을 통해 이러한 데이터를 수집할 수 있다. 크롤링 혹은 스크래핑이란 웹사이트에서 원하는 정보를 수집하는 기술이다. 이번 장에서는 크롤링에 대한 간단한 설명과 예제를 살펴보겠다.\n",
    "\n",
    "```{note}\n",
    "크롤링을 할 때 주의해야 할 점이 있다. 특정 웹사이트의 페이지를 쉬지 않고 크롤링하는 행위를 무한 크롤링이라고 한다. 무한 크롤링은 해당 웹사이트의 자원을 독점하게 되어 타인의 사용을 막게 되며 웹사이트에 부하를 준다. 일부 웹사이트에서는 동일한 IP로 쉬지 않고 크롤링을 할 경우 접속을 막아버리는 경우도 있다. 따라서 하나의 페이지를 크롤링한 후 1~2초 가량 정지하고 다시 다음 페이지를 크롤링하는 것이 좋다.\n",
    "\n",
    "또한 신문기사나 책, 논문, 사진 등 저작권이 있는 자료를 통해 부당이득을 얻는다는 등의 행위를 할 경우 법적 제재를 받을 수 있다. \n",
    "\n",
    "이 책에서 설명하는 크롤링을 통해, 상업적 가치가 있는 데이터에 접근을 시도하여 발생할 수 있는 어떠한 상황에 대해서도 책임을 질 수 없다는 점을 명심하기 바란다.\n",
    "```\n",
    "\n",
    "## GET과 POST 방식 이해하기\n",
    "\n",
    "우리가 인터넷에 접속해 서버에 파일을 요청(Request)하면, 서버는 이에 해당하는 파일을 우리에게 보내준다(Response). 크롬과 같은 웹 브라우저는 이러한 과정을 사람이 수행하기 편하고 시각적으로 보기 편하도록 만들어진 것이며, 인터넷 주소는 서버의 주소를 기억하기 쉽게 만든 것이다. 우리가 서버에 데이터를 요청하는 형태는 다양하지만 크롤링에서는 주로 GET과 POST 방식을 사용한다.\n",
    "\n",
    "```{figure} image/crawl_basic/flow.png\n",
    "---\n",
    "name: flow\n",
    "---\n",
    "클라이언트와 서버 간의 요청/응답 과정\n",
    "```\n",
    "\n",
    "### GET 방식\n",
    "\n",
    "GET 방식은 인터넷 주소를 기준으로 이에 해당하는 데이터나 파일을 요청하는 것이다. 주로 클라이언트가 요청하는 쿼리를 앰퍼샌드(&) 혹은 물음표(?) 형식으로 결합해 서버에 전달한다.\n",
    "\n",
    "네이버 홈페이지에 접속한 후 [퀀트]를 검색하면, 주소 끝부분에 [&query=퀀트]가 추가되며 이에 해당하는 페이지의 내용을 보여준다. 즉, 해당 페이지는 GET 방식을 사용하고 있으며 입력 종류는 query, 입력값은 퀀트임을 알 수 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/naver_search_1.png\n",
    "---\n",
    "name: naver_search_1\n",
    "---\n",
    "네이버 검색 결과\n",
    "```\n",
    "\n",
    "[헤지펀드]를 다시 검색하면, 주소 끝부분이 [&query=헤지펀드&oquery=퀀트...]로 변경된다. 현재 입력값은 헤지펀드, 기존 입력값은 퀀트이며 이러한 과정을 통해 연관검색어가 생성됨도 유추해볼 수 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/naver_search_2.png\n",
    "---\n",
    "name: naver_search_2\n",
    "---\n",
    "네이버 재검색 결과\n",
    "```\n",
    "\n",
    "### POST 방식\n",
    "\n",
    "POST 방식은 사용자가 필요한 값을 추가해서 요청하는 방법이다. GET 방식과 달리 클라이언트가 요청하는 쿼리를 body에 넣어서 전송하므로 요청 내역을 직접 볼 수 없다. 동행복권 홈페이지에 접속해 [당첨결과] 메뉴를 확인해보자.\n",
    "\n",
    "- https://www.dhlottery.co.kr/gameResult.do?method=byWin\n",
    "\n",
    "```{figure} image/crawl_basic/lotto.png\n",
    "---\n",
    "name: lotto\n",
    "---\n",
    "회차별 당첨번호\n",
    "```\n",
    "\n",
    "이번엔 회차 바로가기를 변경한 후 [조회]를 클릭한다. 페이지의 내용은 선택일 기준으로 변경되었지만, 주소는 변경되지 않고 그대로 남아 있다. GET 방식에서는 입력 항목에 따라 웹페이지 주소가 변경되었지만, POST 방식을 사용해 서버에 데이터를 요청하는 해당 웹사이트는 그렇지 않은 것을 알 수 있다.\n",
    "\n",
    "POST 방식의 데이터 요청 과정을 살펴보려면 개발자도구를 이용해야 하며, 크롬에서는 [F12]키를 눌러 개발자도구 화면을 열 수 있다. 개발자도구 화면을 연 상태에서 다시 한번 [조회]를 클릭해보자. [Network] 탭을 클릭하면, [조회]을 클릭함과 동시에 브라우저와 서버 간의 통신 과정을 살펴볼 수 있다. 이 중 상단의 gameResult.do?method=byWin 이라는 항목이 POST 형태임을 알 수 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/lotto_post.png\n",
    "---\n",
    "name: lotto_post\n",
    "---\n",
    "크롬 개발자도구의 Network 화면\n",
    "```\n",
    "\n",
    "해당 메뉴를 클릭하면 통신 과정을 좀 더 자세히 알 수 있다. [Payload] 탭의 [Form Data]에는 서버에 데이터를 요청하는 내역이 있다. drwNo와 dwrNoList에는 선택한 회차의 숫자가 들어가있다.\n",
    "\n",
    "\n",
    "```{figure} image/crawl_basic/lotto_query.png\n",
    "---\n",
    "name: lotto_query\n",
    "---\n",
    "POST 방식의 서버 요청 내역\n",
    "```\n",
    "\n",
    "이처럼 POST 방식은 요청하는 데이터에 대한 쿼리가 GET 방식처럼 URL을 통해 전송되는 것이 아닌 body를 통해 전송되므로, 이에 대한 정보는 웹 브라우저를 통해 확인할 수 없으며, 개발자도구 화면을 통해 확인해야 한다.\n",
    "\n",
    "## 크롤링 예제\n",
    "\n",
    "일반적으로 크롤링은 {numref}`flowchart`의 과정을 따른다. 먼저, request 패키지의 `get()` 혹은 `post()` 함수를 이용해 데이터를 요청한 후 HTML을 정보를 가져오며, bs4 패키지의 함수들을 이용해 원하는 데이터를 찾는 과정으로 이루어진다. 기본적인 크롤링을 시작으로 GET 방식과 POST 방식으로 데이터를 받는 예제를 학습해 보겠다.\n",
    "\n",
    "```{figure} image/crawl_basic/flowchart.png\n",
    "---\n",
    "name: flowchart\n",
    "---\n",
    "일반적인 크롤링 과정\n",
    "```\n",
    "\n",
    "### 명언 크롤링하기\n",
    "\n",
    "크롤링의 간단한 예제로 'Quotes to Scrape' 사이트에 있는 명언을 수집하겠다.\n",
    "\n",
    "```\n",
    "https://quotes.toscrape.com/\n",
    "```\n",
    "\n",
    "해당사이트에 접속한 후, 명언에 해당하는 부분에 마우스 커서를 올려둔 후 마우스 오른쪽 버튼을 클릭하고 [검사]를 선택하면 개발자도구 화면이 나타난다. 여기서 해당 글자가 HTML 내에서 어떤 부분에 위치하는지 확인할 수 있다.\n",
    "\n",
    "- 각 네모에 해당하는 부분: [class가 quote인 div 태그]\n",
    "- 명언: 위의 태그 하부의 [class가 text인 span 태그]\n",
    "- 말한 사람은 [span 태그 하단의 class가 author인 small 태그]\n",
    "- 말한 사람에 대한 정보인 about의 링크: [a 태그 href 속성]의 속성값\n",
    "\n",
    "```{figure} image/crawl_basic/quote.png\n",
    "---\n",
    "name: quote\n",
    "---\n",
    "Quotes to Scrape의 명언부분 HTML\n",
    "```\n",
    "\n",
    "이제 위의 내용을 하나씩 크롤링 해보도록 하자. 먼저 해당 페이지의 내용을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requests 라이브러리를 rq라는 별칭으로 임포트 (HTTP 요청을 위해 사용)\n",
    "import requests as rq\n",
    "\n",
    "# 크롤링할 대상 웹사이트의 URL 설정\n",
    "url = 'https://quotes.toscrape.com/'\n",
    "\n",
    "# 지정한 URL에 HTTP GET 요청을 보내고 응답(Response 객체)을 quote 변수에 저장\n",
    "quote = rq.get(url)\n",
    "\n",
    "# 응답 객체 확인 (Jupyter Notebook 등에서는 응답 상태코드 출력됨, 예: <Response [200]>)\n",
    "quote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url에 해당 주소를 입력한 후 `get()` 함수를 이용해 해당 페이지의 내용을 받았다. 이를 확인해보면 Response가 200, 즉 데이터가 이상 없이 받아졌음이 확인된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<title>Quotes to Scrape</title>\\n    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\\n    <link rel=\"stylesheet\" href=\"/static/main.css\">\\n    \\n    \\n</head>\\n<body>\\n    <div class=\"container\">\\n        <div class=\"row header-box\">\\n            <div class=\"col-md-8\">\\n                <h1>\\n                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\\n                </h1>\\n            </div>\\n            <div class=\"col-md-4\">\\n                <p>\\n                \\n                    <a href=\"/login\">Login</a>\\n                \\n                </p>\\n            </div>\\n        </div>\\n    \\n\\n<div class=\"row\">\\n    <div class=\"col-md-8\">\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답의 HTML 본문을 byte(바이트) 형태로 확인\n",
    "# 전체 내용에서 20번째부터 99번째 바이트까지 일부만 슬라이싱해 확인\n",
    "quote.content[20:100]\n",
    "\n",
    "# HTML 전체 내용을 byte 형태로 출력 (페이지 전체가 출력됨)\n",
    "quote.content\n",
    "\n",
    "# HTML 전체 내용 중 처음 1000바이트만 출력 (페이지 내용의 앞부분만 미리 보기 용도)\n",
    "quote.content[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content`를 통해 함수를 통해 받아온 내용을 확인할 수 있으며, 텍스트 형태로 이루어져있다. `BeautifulSoup()` 함수를 이용해 원하는 HTML 요소에 접근하기 쉬운 BeautifulSoup 객체로 변경할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta charset=\"utf-8\"/>,\n",
       " <title>Quotes to Scrape</title>,\n",
       " <link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>,\n",
       " <link href=\"/static/main.css\" rel=\"stylesheet\"/>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BeautifulSoup 라이브러리에서 BeautifulSoup 클래스를 bs라는 별칭으로 불러오기\n",
    "# 웹 페이지의 HTML 구조를 파싱(분석)할 때 사용됨\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# HTTP 응답 객체인 quote의 내용을 'lxml' 파서를 이용해 HTML 구조로 변환\n",
    "# quote.content는 바이트 형태의 HTML 코드이며, bs() 함수로 파싱하면 HTML 트리 구조가 만들어짐\n",
    "quote_html = bs(quote.content, 'lxml')\n",
    "\n",
    "# 전체 HTML 문서를 BeautifulSoup 객체 형태로 출력\n",
    "# 웹 페이지 전체 구조를 담고 있으며, 콘솔에서 이 변수를 입력하면 HTML 구조가 출력됨\n",
    "quote_html\n",
    "\n",
    "# HTML 문서 중 <head> 태그만 추출\n",
    "# <head>는 페이지 제목(title), 메타 정보(meta), CSS/JS 포함 정보를 담고 있음\n",
    "quote_html.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup()` 함수 내에 HTML 정보에 해당하는 `quote.content`와 파싱 방법에 해당하는 `html.parser`를 입력하면 개발자도구 화면에서 보던 것과 비슷한 형태인 BeautifulSoup 객체로 변경되며, 이를 통해 원하는 요소의 데이터를 읽어올 수 있다.\n",
    "\n",
    "```{note}\n",
    "`BeautifulSoup()` 함수는 다양한 파서를 지원하며, 그 내용은 다음과 같다.\n",
    "\n",
    "| Parser | 선언방법 | 장점 | 단점 |\n",
    "| --- | --- | --- | --- |\n",
    "| html.parser | `BeautifulSoup(내용, 'html.parser')` | 설치할 필요 없음 <br> 적당한 속도 | \n",
    "| lxml HTML parser | `BeautifulSoup(내용, 'lxml')` | 매우 빠름 | lxml 추가 설치 필요 |\n",
    "| lxml XML parser | `BeautifulSoup(내용, 'xml')` | 매우 빠름 <br> 유일하게 XML 파싱 | lxml 추가 설치 필요 |\n",
    "| html5lib | `BeautifulSoup(내용, 'html5lib')` | 웹 브라우저와 같은 방식으로 페이지 파싱. <br> 유효한 HTML5 생성 | html5lib 추가 설치 필요 <br> 매우 느림 |\n",
    "```\n",
    "\n",
    "#### `find()` 함수를 이용한 크롤링\n",
    "\n",
    "먼저 BeautifulSoup 모듈의 `find()` 함수를 통해 크롤링 하는법을 알아보자. 우리는 개발자도구 화면에서 명언에 해당하는 부분이 [class가 quote인 div 태그 → class가 text인 span 태그]에 위치하고 있음을 살펴보았다. 이를 활용해 명언만을 추출하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
       "<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
       "<span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
       "<a href=\"/author/Albert-Einstein\">(about)</a>\n",
       "</span>\n",
       "<div class=\"tags\">\n",
       "            Tags:\n",
       "            <meta class=\"keywords\" content=\"change,deep-thoughts,thinking,world\" itemprop=\"keywords\"/>\n",
       "<a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
       "<a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
       "<a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
       "<a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quote_html 객체에서 <div> 태그 중 class가 'quote'인 첫 번째 요소만 찾기\n",
    "# find()는 일치하는 첫 번째 요소 하나만 반환함\n",
    "quote_div = quote_html.find('div', class_='quote')\n",
    "\n",
    "# 방금 찾은 첫 번째 quote 요소를 출력 (BeautifulSoup 객체 형태로 출력됨)\n",
    "quote_div\n",
    "\n",
    "# quote_html에서 <div> 태그 중 class가 'quote'인 **모든 요소**를 찾아 리스트로 반환\n",
    "# find_all()은 조건에 맞는 모든 태그를 리스트로 반환함\n",
    "quote_div = quote_html.find_all('div', class_='quote')\n",
    "\n",
    "# 찾은 모든 <div class='quote'> 요소들을 리스트 형태로 출력\n",
    "# 이 리스트는 명언 하나하나의 블록(div)을 각각 요소로 가짐\n",
    "quote_div\n",
    "\n",
    "# 리스트에서 첫 번째 명언 블록(div)을 꺼내어 출력\n",
    "# 즉, 첫 번째 quote 박스 안의 내용을 보고 싶은 경우 사용\n",
    "quote_div[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all()` 함수를 이용할 경우 원하는 태그의 내용들을 찾아올 수 있다. 먼저 태그에 해당하는 'div'를 입력하고, class 이름인 'quote'를 입력한다. class라는 키워드는 파이썬에서 클래스를 만들 때 사용하는 키워드이므로 언더바(\\_)를 통해 중복을 피해준다. 조건에 만족하는 결과가 리스트 형태로 반환되므로, 첫번째 내용만 확인해보면 `div class=\"quote\"`에 해당하는 내용을 찾아왔으며, 이제 여기서 [class가 text인 span 태그]에 해당하는 내용을 추가로 찾도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quote_div[0]은 첫 번째 명언 블록(div 태그)을 의미함\n",
    "# 그 안에서 <span> 태그 중 class가 'text'인 요소를 모두 찾아 리스트로 저장\n",
    "# 여기서 'text' 클래스는 명언 내용(문장)을 담고 있음\n",
    "quote_span = quote_div[0].find_all('span', class_='text')\n",
    "\n",
    "# 찾은 결과 출력\n",
    "# 명언 문장들이 들어 있는 <span class=\"text\"> 태그들이 리스트로 출력됨\n",
    "quote_span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 한번 `find_all()` 함수를 이용해 원하는 부분(`'span', class_='text'`)을 입력하면 우리가 원하던 명언에 해당하는 내용이 찾아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_span[0].text \n",
    "# <span> 명언 글자 </span> 형태에서 명언 글자 가지오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과물 마지막에 `.text`를 입력하면 텍스트 데이터만을 출력할 수 있다. for문 중에서 리스트 내포 형태를 이용하여 명언에 해당하는 부분을 한번에 추출해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
       " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
       " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
       " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
       " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
       " '“Try not to become a man of success. Rather become a man of value.”',\n",
       " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
       " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
       " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
       " '“A day without sunshine is like, you know, night.”']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "명언 = []\n",
    "for i in quote_div:\n",
    "    spans = i.find_all('span', class_='text')\n",
    "    if spans:\n",
    "       명언.append( spans[0].text )\n",
    "명언\n",
    "\n",
    "명언2 = [ i.find_all('span', class_='text')[0].text for i in quote_div ]\n",
    "#      3번                    2번                           1번\n",
    "# 1번은 반복문\n",
    "# 2번은 반복문의 내용\n",
    "# 3번은 리스트 저장\n",
    "명언2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`find_all()` 함수가 아닌 `find()` 함수를 사용하면 해당 태그의 첫번째 내용만을 가져온다.\n",
    "```\n",
    "\n",
    "#### `select()` 함수를 이용한 크롤링\n",
    "\n",
    "위 예제에서는 간단하게 원하는 데이터를 찾았지만, 데이터가 존재하는 곳의 태그를 여러번 찾아 내려가야 할 경우 `find_all()` 함수를 이용하는 방법은 매우 번거롭다. `select()` 함수의 경우 좀더 쉬운 방법으로 원하는 데이터가 존재하는 태그를 입력할 수 있다. 위의 동일한 내용을 `select()` 함수를 이용해 크롤링해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
       " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
       " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
       " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
       " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
       " '“Try not to become a man of success. Rather become a man of value.”',\n",
       " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
       " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
       " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
       " '“A day without sunshine is like, you know, night.”']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "명언 = [i.find_all('span',class_='text')[0].text for i in quote_div]\n",
    "명언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select()` 함수 내에 찾고자 하는 태그를 입력하며, 클래스명이 존재할 경우 점(.)을 붙여준다. 또한 여러 태그를 찾아 내려가야할 경우 `>` 기호를 이용해 순서대로 입력해주면 된다. 즉 'div.quote > span.text'는 [class가 quote인 div 태그] 중에서 [class가 text인 span 태그]를 찾는다. 이제 텍스트 데이터만 추출해보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“I have not failed. I've just found 10,000 ways that won't work.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it's in hot water.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_text = quote_html.select('div.quote > span.text')\n",
    "quote_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all()` 함수를 이용한 것 보다 훨씬 간단하게 원하는 데이터를 찾을 수 있었다.\n",
    "\n",
    "이번에는 명언을 말한 사람 역시 크롤링해보도록 하자. 해당 데이터는 [class가 quote인 div 태그] 하단의 [span 태그], 다시 하단의 [class가 author인 small 태그]에 위치하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
       " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
       " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
       " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
       " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
       " '“Try not to become a man of success. Rather become a man of value.”',\n",
       " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
       " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
       " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
       " '“A day without sunshine is like, you know, night.”']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_text_list = [i.text for i in quote_text]\n",
    "quote_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 동일한 방법을 이용해 말한 사람 역시 손쉽게 추출이 가능합니다.\n",
    "\n",
    "마지막으로 말한 사람에 대한 정보인 (about)에 해당하는 링크도 추출해보자. 해당 주소는 [class가 quote인 div 태그] 하단의 [span 태그], 다시 하단의 [a 태그의 href 속성] 중 속성값에 위치하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert Einstein',\n",
       " 'J.K. Rowling',\n",
       " 'Albert Einstein',\n",
       " 'Jane Austen',\n",
       " 'Marilyn Monroe',\n",
       " 'Albert Einstein',\n",
       " 'André Gide',\n",
       " 'Thomas A. Edison',\n",
       " 'Eleanor Roosevelt',\n",
       " 'Steve Martin']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_author = quote_html.select('div.quote > span > small.author')\n",
    "quote_author_text = [ i.text  for i in quote_author]\n",
    "quote_author_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 중에서 우리는 속성값에 해당하는 정보만 필요하다. 속성값의 경우 HTML 정보 뒤에 ['속성']을 입력하면 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/author/Albert-Einstein\">(about)</a>,\n",
       " <a href=\"/author/J-K-Rowling\">(about)</a>,\n",
       " <a href=\"/author/Albert-Einstein\">(about)</a>,\n",
       " <a href=\"/author/Jane-Austen\">(about)</a>,\n",
       " <a href=\"/author/Marilyn-Monroe\">(about)</a>,\n",
       " <a href=\"/author/Albert-Einstein\">(about)</a>,\n",
       " <a href=\"/author/Andre-Gide\">(about)</a>,\n",
       " <a href=\"/author/Thomas-A-Edison\">(about)</a>,\n",
       " <a href=\"/author/Eleanor-Roosevelt\">(about)</a>,\n",
       " <a href=\"/author/Steve-Martin\">(about)</a>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_link = quote_html.select('div.quote > span > a')\n",
    "quote_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 속성값을 한 번에 추출한 후, 완전한 URL을 만들기 위해 주소 부분도 합쳐주도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/author/Albert-Einstein',\n",
       " '/author/J-K-Rowling',\n",
       " '/author/Albert-Einstein',\n",
       " '/author/Jane-Austen',\n",
       " '/author/Marilyn-Monroe',\n",
       " '/author/Albert-Einstein',\n",
       " '/author/Andre-Gide',\n",
       " '/author/Thomas-A-Edison',\n",
       " '/author/Eleanor-Roosevelt',\n",
       " '/author/Steve-Martin']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_link[0]['href']\n",
    "# 모든 속성을 출력해 보세요 리스트컴프리헨션을 이용해서\n",
    "quote_link_list = [ i['href'] for i in quote_link ]\n",
    "quote_link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 페이지 데이터 크롤링하기\n",
    "\n",
    "화면 하단의 [Next→] 부분을 클릭하면 URL이 https://quotes.toscrape.com/page/2/ 로 바뀌며 다음 페이지의 내용이 나타난다. 이처럼 웹페이지 하단에서 다음 페이지 혹은 이전 페이지로 넘어가게 해주는 것을 흔히 페이지네이션이라고 한다. \n",
    "\n",
    "```{figure} image/crawl_basic/pagination.png\n",
    "---\n",
    "name: pagination\n",
    "---\n",
    "페이지네이션\n",
    "```\n",
    "\n",
    "URL의 'page/' 뒤에 위치하는 숫자를 for문을 이용해 바꿔준다면, 모든 페이지의 데이터를 크롤링할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://quotes.toscrape.com/author/Albert-Einstein',\n",
       " 'https://quotes.toscrape.com/author/J-K-Rowling',\n",
       " 'https://quotes.toscrape.com/author/Albert-Einstein',\n",
       " 'https://quotes.toscrape.com/author/Jane-Austen',\n",
       " 'https://quotes.toscrape.com/author/Marilyn-Monroe',\n",
       " 'https://quotes.toscrape.com/author/Albert-Einstein',\n",
       " 'https://quotes.toscrape.com/author/Andre-Gide',\n",
       " 'https://quotes.toscrape.com/author/Thomas-A-Edison',\n",
       " 'https://quotes.toscrape.com/author/Eleanor-Roosevelt',\n",
       " 'https://quotes.toscrape.com/author/Steve-Martin']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_link_list = ['https://quotes.toscrape.com' + i['href'] for i in quote_link ]\n",
    "quote_link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 명언과 말한 사람, 링크가 들어갈 빈 리스트(text_list, author_list, infor_list)를 만든다.\n",
    "2. for문을 1부터 100까지 적용하여 URL을 생성한다.\n",
    "3. HTML 정보를 받아온 후 `BeautifulSoup()` 함수를 통해 파싱한다.\n",
    "4. 명언과 말한 사람, 링크에 해당하는 내용을 각각 추출한다.\n",
    "5. 해당 웹페이지는 10페이지까지 데이터가 존재하며, 11페이지부터는 아무런 내용이 없다. 그러나 이러한 정보는 사전에 알 수 없기에 만약 데이터가 있는 경우 위에서 생성한 리스트에 `extend()` 함수를 사용하여 데이터를 추가하며, 그렇지 않을 경우 `break`를 통해 for문을 종료한다.\n",
    "6. 한 번 루프가 돌때마다 1초간 정지를 준다.\n",
    "\n",
    "text_list와 author_list, infor_list를 확인해보면 모든 페이지의 내용이 저장되어 있다. 이제 크롤링 한 내용을 데이터프레임 형태로 만들도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "text_list = [] #모든 페이지 명언 리스트\n",
    "author_list = [] #모든 페이지 작성자 리스트\n",
    "infor_list = [] #모든 페이지 사이트 링크 리스트\n",
    "\n",
    "for i in range(1, 100):\n",
    "    url = f'https://quotes.toscrape.com/page/{i}/'\n",
    "    quote = rq.get(url)\n",
    "    quote_html = bs(quote.content, 'lxml')\n",
    "    quote_text = quote_html.select('div.quote > span.text')\n",
    "    quote_text_list = [i.text for i in quote_text] #한개의 명언 \n",
    "    quote_author = quote_html.select('div.quote > span > small.author')\n",
    "    quote_author_list = [i.text for i in quote_author] # 한명의 작성자\n",
    "    # 작성자를 이용한 링크 주소\n",
    "    quote_link = quote_html.select('div.quote > span > a')\n",
    "    quote_link_list=[ 'https://quotes.toscrape.com'+i['href'] for i in quote_link ]\n",
    "\n",
    "    if len(quote_text_list) > 0 :\n",
    "        text_list.extend( quote_text_list )\n",
    "        author_list.extend( quote_author_list )\n",
    "        infor_list.extend( quote_link_list )\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 금융 속보 크롤링\n",
    "\n",
    "이번에는 금융 속보의 제목을 추출해보겠다. 먼저 네이버 금융에 접속한 후 [뉴스 → 실시간 속보]를 선택하며, URL은 다음과 같다. \n",
    "\n",
    "```\n",
    "https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258\n",
    "```\n",
    "\n",
    "이 중 뉴스의 제목에 해당하는 텍스트만 추출해보도록 하자. 개발자도구 화면을 통헤 제목에 해당하는 부분은 [dl 태그 → class가 articleSubject 인 dd 태그 → a 태그 중 title 속성]에 위치하고 있음을 확인할 수 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/naver_news.png\n",
    "---\n",
    "name: naver_news\n",
    "---\n",
    "실시간 속보의 제목 부분 HTML\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quote_a2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m quote_html \u001b[38;5;241m=\u001b[39m bs( quote\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m      8\u001b[0m quote_a\u001b[38;5;241m=\u001b[39mquote_html\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdl > dd.articleSubject > a\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mquote_a2\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quote_a2' is not defined"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "url = 'https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258'\n",
    "quote = rq.get(url)\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "quote_html = bs( quote.content, 'lxml' )\n",
    "\n",
    "quote_a=quote_html.select('dl > dd.articleSubject > a')\n",
    "quote_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `get()` 함수를 이용해 페이지의 내용을 받아온다.\n",
    "2. `BeautifulSoup()` 함수를 통해 HTML 정보를 BeautifulSoup 객체로 만든다.\n",
    "3. `select()` 함수를 통해 원하는 태그로 접근해 들어간다. \n",
    "\n",
    "출력된 내용을 살펴 보면 우리가 원하는 제목은 title 속성에 위치하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'신한은행 \"신탁형 ISA 수탁고 5조원 넘어\"'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_a[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "속성값에 해당하는 내용을 추출했다. 이제 for문으로 묶어 한번에 제목들을 추출하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>신한은행 \"신탁형 ISA 수탁고 5조원 넘어\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>유안타증권 'EDC KOREA 2025' 스폰서 참여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MG캐피탈 2천억 유증…8.5% 고금리 대출 상환 나선다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이달 '투자경고·위험' 종목 80%는 정치테마주…\"불공정거래 강력대처\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5년만에 역성장...편의점, 해외 노린다 [마켓딥다이브]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>딥노이드, 연세대 공과대학과 의료영상·AI 공동 연구 협약 체결</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>한국경제TV, 찾아가는 해외주식 무료 현장세미나 26일 개최</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>트럼프 관세전쟁에 '셀 아메리카'…유럽펀드로 갈아타는 투자자들</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>증여 소식에 한미반도체 13% 상승 '훨훨' [줍줍리포트]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>김소영 금융위 부위원장, ESG 금융추진단 제5차 회의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>김소영 금융위 부위원장, ESG 금융추진단 제5차 회의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>김소영 금융위 부위원장, ESG 금융추진단 제5차 회의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ESG 금융추진단 제5차 회의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>김소영 금융위 부위원장, ESG 금융추진단 제5차 회의</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>네이버 \"외산 AI, 상표만 갈아끼운다고 소버린 아냐\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>더그리트, 서울시 다회용기ㆍ다회용컵 지원 사업자로 선정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>코레일네트웍스, 직무급 고도화 노·사 합의 체결</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[마켓칼럼] \"풍산, 나홀로 저평가…인적분할 시 기업가치 두 배 뛴다\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>검찰, '유상증자' 고려아연 압수수색</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title\n",
       "0                 신한은행 \"신탁형 ISA 수탁고 5조원 넘어\"\n",
       "1             유안타증권 'EDC KOREA 2025' 스폰서 참여\n",
       "2           MG캐피탈 2천억 유증…8.5% 고금리 대출 상환 나선다\n",
       "3   이달 '투자경고·위험' 종목 80%는 정치테마주…\"불공정거래 강력대처\"\n",
       "4           5년만에 역성장...편의점, 해외 노린다 [마켓딥다이브]\n",
       "5       딥노이드, 연세대 공과대학과 의료영상·AI 공동 연구 협약 체결\n",
       "6         한국경제TV, 찾아가는 해외주식 무료 현장세미나 26일 개최\n",
       "7        트럼프 관세전쟁에 '셀 아메리카'…유럽펀드로 갈아타는 투자자들\n",
       "8          증여 소식에 한미반도체 13% 상승 '훨훨' [줍줍리포트]\n",
       "9            김소영 금융위 부위원장, ESG 금융추진단 제5차 회의\n",
       "10           김소영 금융위 부위원장, ESG 금융추진단 제5차 회의\n",
       "11           김소영 금융위 부위원장, ESG 금융추진단 제5차 회의\n",
       "12                         ESG 금융추진단 제5차 회의\n",
       "13           김소영 금융위 부위원장, ESG 금융추진단 제5차 회의\n",
       "14           네이버 \"외산 AI, 상표만 갈아끼운다고 소버린 아냐\"\n",
       "15           더그리트, 서울시 다회용기ㆍ다회용컵 지원 사업자로 선정\n",
       "16               코레일네트웍스, 직무급 고도화 노·사 합의 체결\n",
       "17  [마켓칼럼] \"풍산, 나홀로 저평가…인적분할 시 기업가치 두 배 뛴다\"\n",
       "18                     검찰, '유상증자' 고려아연 압수수색"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_title = [i['title'] for i in quote_a]\n",
    "quote_title\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'title' : quote_title})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테이블 크롤링하기\n",
    "\n",
    "우리가 크롤링하고자 하는 데이터가 테이블 형태로 제공될 경우, 위와 같이 복잡한 과정을 거칠 필요 없이 매우 간단하게 테이블에 해당하는 내용만 가져올 수 있다. 먼저 아래 사이트에는 각 국가별 GDP가 테이블 형태로 제공되고 있다.\n",
    "\n",
    "```\n",
    "https://en.wikipedia.org/wiki/List_of_countries_by_stock_market_capitalization\n",
    "```\n",
    "\n",
    "```{figure} image/crawl_basic/cap.png\n",
    "---\n",
    "name: cap\n",
    "---\n",
    "국가별 시가총액 데이터\n",
    "```\n",
    "\n",
    "해당 내역을 크롤링하는 법은 매우 간단하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    Country / Territory        Total market cap                     \\\n",
       "     Country / Territory (in millions of US$)[3] (as a % of GDP)[4]   \n",
       " 0         United States                56549774              206.7   \n",
       " 1                 China           14,411,090[5]               65.1   \n",
       " 2                 Japan           5,504,375 [7]           146.2[8]   \n",
       " 3             Hong Kong           5,463,067 [9]             1262.6   \n",
       " 4                 India           5,063,221[10]              140.1   \n",
       " ..                  ...                     ...                ...   \n",
       " 101            Paraguay                     313                3.5   \n",
       " 102             Uruguay                     284                1.4   \n",
       " 103             Armenia                     253                1.3   \n",
       " 104            Eswatini                     234                6.8   \n",
       " 105             Bermuda                     207                2.7   \n",
       " \n",
       "     Number of domestic companies listed[2]  Year  \n",
       "     Number of domestic companies listed[2]  Year  \n",
       " 0                                     4642  2025  \n",
       " 1                                 5,363[6]  2024  \n",
       " 2                                     3865  2024  \n",
       " 3                                     2414  2024  \n",
       " 4                                5,451[11]  2024  \n",
       " ..                                     ...   ...  \n",
       " 101                                     55  1999  \n",
       " 102                                     17  1996  \n",
       " 103                                     12  2022  \n",
       " 104                                      6  2007  \n",
       " 105                                     13  2022  \n",
       " \n",
       " [106 rows x 5 columns],\n",
       "     Year World market cap          Number of listed companies\n",
       "     Year  Millions of US$ % of GDP Number of listed companies\n",
       " 0   1975          1149245     27.2                      14577\n",
       " 1   1980          2525736     29.6                      17273\n",
       " 2   1985          4684978     47.0                      20555\n",
       " 3   1990          9519107     50.8                      23732\n",
       " 4   1991         11340785     56.8                      24666\n",
       " 5   1992         10819256     50.2                      24947\n",
       " 6   1993         13897390     61.7                      28300\n",
       " 7   1994         14639924     60.9                      30290\n",
       " 8   1995         17263728     64.0                      33379\n",
       " 9   1996         19806691     72.3                      35617\n",
       " 10  1997         22029761     80.7                      36946\n",
       " 11  1998         24555201     89.6                      37928\n",
       " 12  1999         33181159    115.1                      38414\n",
       " 13  2000         30925434    101.1                      39892\n",
       " 14  2001         26792162     88.4                      40157\n",
       " 15  2002         22802792     72.7                      38894\n",
       " 16  2003         31107425     84.9                      41051\n",
       " 17  2004         36540980     89.2                      38724\n",
       " 18  2005         40512446     92.6                      39096\n",
       " 19  2006         50074966    106.1                      43104\n",
       " 20  2007         60456082    114.0                      44034\n",
       " 21  2008         32418516     56.2                      43949\n",
       " 22  2009         47471293     83.8                      42669\n",
       " 23  2010         54259518     87.3                      43427\n",
       " 24  2011         47521341     68.8                      44323\n",
       " 25  2012         54503237     78.4                      43772\n",
       " 26  2013         64367842     89.0                      44853\n",
       " 27  2014         67177254     90.3                      45743\n",
       " 28  2015         62268184     94.5                      43983\n",
       " 29  2016         65117714     97.1                      43806\n",
       " 30  2017         79501948    111.1                      43440\n",
       " 31  2018         68893044     91.9                      45050\n",
       " 32  2019         78825583    108.4                      49921\n",
       " 33  2020         93686226    133.4                      49839\n",
       " 34  2021        111159259    131.8                      51337\n",
       " 35  2022         93688922    106.2                      47926,\n",
       "               vteEconomic classification of countries  \\\n",
       " 0   Developed country Developing country Heavily i...   \n",
       " 1                              Three/Four-World Model   \n",
       " 2                        Gross domestic product (GDP)   \n",
       " 3                                             Nominal   \n",
       " 4                       Purchasing power parity (PPP)   \n",
       " 5                                              Income   \n",
       " 6                                               Wages   \n",
       " 7                                              Wealth   \n",
       " 8                             Other national accounts   \n",
       " 9                                   Human development   \n",
       " 10                                     Digital divide   \n",
       " 11       Net international investment position (NIIP)   \n",
       " 12                     Economics portal  World portal   \n",
       " \n",
       "             vteEconomic classification of countries.1  \n",
       " 0   Developed country Developing country Heavily i...  \n",
       " 1   First World Second World Third World Fourth World  \n",
       " 2   Nominal By country past and projected per capi...  \n",
       " 3   By country past and projected per capita per c...  \n",
       " 4   By country future estimates per capita per cap...  \n",
       " 5   GNI (nominal) per capita GNI (PPP) per capita ...  \n",
       " 6   Average wage Europe Employee compensation (per...  \n",
       " 7   Wealth per adult Europe Financial assets per c...  \n",
       " 8   Gross National Happiness Net material product ...  \n",
       " 9   Human Development Index by country inequality-...  \n",
       " 10  ICT Development Index Number of broadband Inte...  \n",
       " 11        Per capita (creditors) Per capita (debtors)  \n",
       " 12                     Economics portal  World portal  ,\n",
       "                                0  \\\n",
       " 0                        Nominal   \n",
       " 1  Purchasing power parity (PPP)   \n",
       " \n",
       "                                                    1  \n",
       " 0  By country past and projected per capita per c...  \n",
       " 1  By country future estimates per capita per cap...  ]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_stock_market_capitalization'\n",
    "df = pd.read_html(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. URL을 입력한다.\n",
    "2. pandas 패키지의 `read_html()` 함수에 URL을 입력하면, 해당 페이지에 존재하는 테이블을 가져온 후 데이터프레임 형태로 불러온다. \n",
    "\n",
    "이처럼 테이블 형태로 존재하는 데이터는 HTML 정보를 불러온 후 태그와 속성을 찾을필요 없이 `read_html()` 함수를 이용해 매우 손쉽게 불러올 수 있다.\n",
    "\n",
    "### 기업공시채널에서 오늘의 공시 불러오기\n",
    "\n",
    "한국거래소 상장공시시스템(kind.krx.co.kr)에 접속한 후 [오늘의 공시 → 전체 → 더보기]를 선택해 전체 공시내용을 확인할 수 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/kind.png\n",
    "---\n",
    "name: kind\n",
    "---\n",
    "오늘의공시 확인하기\n",
    "```\n",
    "\n",
    "해당 페이지에서 날짜를 변경한 후 [검색]을 누르면, 페이지의 내용은 해당일의 공시로 변경되지만 URL은 변경되지 않는다. 이처럼 POST 방식은 요청하는 데이터에 대한 쿼리가 body의 형태를 통해 전송되므로, 개발자도구 화면을 통해 해당 쿼리에 대한 내용을 확인해야 한다.\n",
    "\n",
    "개발자도구 화면을 연 상태에서 조회일자를 원하는 날짜로 선택, [검색]을 클릭한 후 [Network] 탭의 todaydisclosure.do 항목에서 [Headers]탭의 [General] 부분에는 데이터를 요청하는 서버 주소가, [Payload] 탭의 [Form Data]를 통해 서버에 데이터를 요청하는 내역을 확인할 수 있다. 여러 항목 중 selDate 부분이 우리가 선택한 일자로 설정되어 있다.\n",
    "\n",
    "```{figure} image/crawl_basic/kind_post.png\n",
    "---\n",
    "name: kind_post\n",
    "---\n",
    "POST 방식의 데이터 요청\n",
    "```\n",
    "\n",
    "POST 방식으로 쿼리를 요청하는 방법을 코드로 나타내면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quote_htmlᄋ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m quote\n\u001b[0;32m     18\u001b[0m quote_html \u001b[38;5;241m=\u001b[39m bs(quote\u001b[38;5;241m.\u001b[39mcontent,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mquote_htmlㅇ\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quote_htmlᄋ' is not defined"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "url = 'https://kind.krx.co.kr/disclosure/todaydisclosure.do?method=searchTodayDisclosureMain&marketType=0'\n",
    "payload = {\n",
    "    'method': 'searchTodayDisclosureSub',\n",
    "    'currentPageSize': 15,\n",
    "    'pageIndex': 1,\n",
    "    'orderMode': 0,\n",
    "    'orderStat': 'D',\n",
    "    'forward': 'todaydisclosure_sub',\n",
    "    'chose': 'S',\n",
    "    'todayFlag': 'N',\n",
    "    'selDate': '2025-04-22'\n",
    " }\n",
    "quote = rq.post(url,data=payload)\n",
    "quote\n",
    "quote_html = bs(quote.content,'lxml')\n",
    "quote_html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} image/crawl_basic/html.png\n",
    "---\n",
    "name: html\n",
    "---\n",
    "```\n",
    "\n",
    "1. URL과 쿼리를 입력한다. 쿼리는 딕셔너리 형태로 입력하며, Form Data와 동일하게 입력해준다. 쿼리 중 marketType과 같이 값이 없는 항목은 입력하지 않아도 된다.\n",
    "2. `POST()` 함수를 통해 해당 URL에 원하는 쿼리를 요청한다.\n",
    "3. `BeautifulSoup()` 함수를 통해 파싱한다.\n",
    "\n",
    "읽어온 데이터를 확인해보면 엑셀 데이터가 HTML 형태로 나타나있다. 따라서 이를 변형해 데이터프레임 형태로 불러오도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `prettify()` 함수를 이용해 BeautifulSoup 에서 파싱한 파서 트리를 유니코드 형태로 다시 돌려준다.\n",
    "2. `read_html()` 함수를 통해 테이블을 읽어온다.\n",
    "\n",
    "데이터를 확인하면 화면과 동일한 내용이 들어가있다. POST 형식의 경우 쿼리 내용을 바꾸어 원하는 데이터를 받을 수 있다. 만일 다른 날짜의 공시를 확인하고자 한다면 위의 코드에서 'selDate'만 해당일로 변경해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
